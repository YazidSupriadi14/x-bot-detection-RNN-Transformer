{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLozkronYyz4",
        "outputId": "3dc07d61-29b5-4fa7-d202-85ef8f8c8f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/yazidsupriadi/bot-detector-lstm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqWhUZ5ZY7uI",
        "outputId": "d4f33e25-6494-41d8-be1d-80bc95dda497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bot-detector-lstm'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 60 (delta 14), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (60/60), 218.91 KiB | 7.82 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/yazidsupriadi/gru_bot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzyxeCpGWwK3",
        "outputId": "980b4ed8-ccc1-4965-fa24-85e243b22e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gru_bot'...\n",
            "remote: Enumerating objects: 216, done.\u001b[K\n",
            "remote: Counting objects: 100% (213/213), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 216 (delta 84), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Receiving objects: 100% (216/216), 263.24 KiB | 11.44 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Filtering content: 100% (3/3), 45.31 MiB | 30.21 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# === Define Model ===\n",
        "class BotDetector(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_numeric, output_dim):\n",
        "        super(BotDetector, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_text = nn.Linear(hidden_dim, 64)\n",
        "        self.fc_numeric = nn.Linear(num_numeric, 32)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, x_num):\n",
        "        x = self.embedding(x_text)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        text_out = self.fc_text(h_n[-1])\n",
        "        num_out = self.fc_numeric(x_num)\n",
        "        combined = torch.cat((text_out, num_out), dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# === Load assets ===\n",
        "with open(\"./bot-detector-lstm/vocab.pkl\", \"rb\") as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "with open(\"./bot-detector-lstm/scaler.pkl\", \"rb\") as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "with open(\"./bot-detector-lstm/label_encoder.pkl\", \"rb\") as f:\n",
        "    le = pickle.load(f)\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "model = BotDetector(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=100,\n",
        "    hidden_dim=128,\n",
        "    num_numeric=5,\n",
        "    output_dim=len(le.classes_)\n",
        ")\n",
        "model.load_state_dict(torch.load(\"./bot-detector-lstm/model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# === Preprocessing ===\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", str(text))\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "def encode(tokens):\n",
        "    return [vocab.get(token, 1) for token in tokens]\n",
        "\n",
        "# === Predict Function ===\n",
        "def predict_bot(text, favorite_count, retweet_count, reply_count, quote_count, tweet_per_day):\n",
        "    clean = clean_text(text)\n",
        "    tokens = tokenize(clean)\n",
        "    encoded = torch.tensor(encode(tokens))\n",
        "    padded = pad_sequence([encoded], batch_first=True)\n",
        "    text_input = padded.to(device)\n",
        "\n",
        "    numeric_input = np.array([[favorite_count, retweet_count, reply_count, quote_count, tweet_per_day]])\n",
        "    numeric_scaled = scaler.transform(numeric_input)\n",
        "    numeric_tensor = torch.tensor(numeric_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(text_input, numeric_tensor)\n",
        "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
        "        pred_idx = np.argmax(probs)\n",
        "        label = le.classes_[pred_idx]\n",
        "        confidence = probs[pred_idx]\n",
        "\n",
        "    return f\"{label} (confidence: {confidence:.2f})\"\n",
        "\n",
        "# === Gradio UI ===\n",
        "inputs = [\n",
        "    gr.Textbox(label=\"Tweet Text\", placeholder=\"Masukkan teks tweet di sini...\"),\n",
        "    gr.Number(label=\"Favorite Count\", value=0),\n",
        "    gr.Number(label=\"Retweet Count\", value=0),\n",
        "    gr.Number(label=\"Reply Count\", value=0),\n",
        "    gr.Number(label=\"Quote Count\", value=0),\n",
        "    gr.Number(label=\"Tweet Per Day\", value=1),\n",
        "]\n",
        "\n",
        "output = gr.Textbox(label=\"Hasil Prediksi\")\n",
        "\n",
        "gr.Interface(\n",
        "    fn=predict_bot,\n",
        "    inputs=inputs,\n",
        "    outputs=output,\n",
        "    title=\"Bot Detector with LSTM\",\n",
        "    description=\"Masukkan teks dan fitur numerik untuk memprediksi apakah tweet berasal dari bot atau manusia.\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "XN_0x92QY_pp",
        "outputId": "c043b82d-425e-42d5-f157-518a61157c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f27ecc612f3a2da985.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f27ecc612f3a2da985.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import joblib\n",
        "\n",
        "# ===== Model Definitions =====\n",
        "class LSTMBotDetector(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_numeric, output_dim):\n",
        "        super(LSTMBotDetector, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_text = nn.Linear(hidden_dim, 64)\n",
        "        self.fc_numeric = nn.Linear(num_numeric, 32)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, x_num):\n",
        "        x = self.embedding(x_text)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        text_out = self.fc_text(h_n[-1])\n",
        "        num_out = self.fc_numeric(x_num)\n",
        "        combined = torch.cat((text_out, num_out), dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "class GRUBotDetector(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_numeric, output_dim):\n",
        "        super(GRUBotDetector, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.bn_text = nn.BatchNorm1d(hidden_dim * 2)\n",
        "        self.fc_text = nn.Linear(hidden_dim * 2, 64)\n",
        "        self.bn_num = nn.BatchNorm1d(num_numeric)\n",
        "        self.fc_numeric = nn.Linear(num_numeric, 32)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(96, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, x_num):\n",
        "        x = self.embedding(x_text)\n",
        "        x = self.dropout(x)\n",
        "        _, h_n = self.gru(x)\n",
        "        h_n = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
        "        text_out = self.bn_text(h_n)\n",
        "        text_out = self.fc_text(text_out)\n",
        "        num_out = self.bn_num(x_num)\n",
        "        num_out = self.fc_numeric(num_out)\n",
        "        combined = torch.cat((text_out, num_out), dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# ===== Load Scaler & Label Encoder =====\n",
        "scaler = joblib.load(\"./bot-detector-lstm/scaler.pkl\")\n",
        "le = joblib.load(\"./bot-detector-lstm/label_encoder.pkl\")\n",
        "\n",
        "# ===== Config =====\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vocab_size_gru = 36380\n",
        "vocab_size_lstm = 36380\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "num_numerical_features = 5\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "# ===== Load Models =====\n",
        "gru_model = GRUBotDetector(vocab_size_gru, embedding_dim, hidden_dim, num_numerical_features, num_classes).to(device)\n",
        "gru_model.load_state_dict(torch.load(\"gru_bot/best_gru_bot_model.pth\", map_location=device))\n",
        "gru_model.eval()\n",
        "\n",
        "lstm_model = LSTMBotDetector(vocab_size_lstm, embedding_dim, hidden_dim, num_numerical_features, num_classes).to(device)\n",
        "lstm_model.load_state_dict(torch.load(\"bot-detector-lstm/model.pth\", map_location=device))\n",
        "lstm_model.eval()\n",
        "\n",
        "# ===== Preprocessing =====\n",
        "def clean_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "def encode(tokens, vocab_size):\n",
        "    return [min(hash(t) % vocab_size, vocab_size - 1) for t in tokens]\n",
        "\n",
        "# ===== Prediction Function =====\n",
        "def predict_bot(username, text, fav, rt, reply, quote, tpd, model_choice):\n",
        "    text = clean_text(text)\n",
        "    tokens = tokenize(text)\n",
        "\n",
        "    vocab_size = vocab_size_gru if model_choice == \"GRU\" else vocab_size_lstm\n",
        "    encoded = torch.tensor([encode(tokens, vocab_size)], dtype=torch.long).to(device)\n",
        "\n",
        "    if encoded.shape[1] < 5:\n",
        "        encoded = F.pad(encoded, (0, 5 - encoded.shape[1]))\n",
        "\n",
        "    numeric = np.array([[fav, rt, reply, quote, tpd]])\n",
        "    numeric = scaler.transform(numeric)\n",
        "    numeric_tensor = torch.tensor(numeric, dtype=torch.float32).to(device)\n",
        "\n",
        "    model = gru_model if model_choice == \"GRU\" else lstm_model\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(encoded, numeric_tensor)\n",
        "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
        "        pred = np.argmax(probs)\n",
        "        label = le.inverse_transform([pred])[0]\n",
        "\n",
        "    # Optional: log or show username (here we just ignore it, but you can use it)\n",
        "    return {le.classes_[i]: float(probs[i]) for i in range(len(probs))}, f\"{label} (username: {username})\"\n",
        "\n",
        "\n",
        "# ===== Gradio Interface =====\n",
        "inputs = [\n",
        "    gr.Textbox(label=\"Username\"),  # <--- Tambahan input\n",
        "    gr.Textbox(label=\"Teks Tweet\"),\n",
        "    gr.Slider(0, 10000, step=1, label=\"favorite_count\"),\n",
        "    gr.Slider(0, 10000, step=1, label=\"retweet_count\"),\n",
        "    gr.Slider(0, 10000, step=1, label=\"reply_count\"),\n",
        "    gr.Slider(0, 10000, step=1, label=\"quote_count\"),\n",
        "    gr.Slider(0, 100, step=0.1, label=\"tweet_per_day\"),\n",
        "    gr.Radio([\"GRU\", \"LSTM\"], label=\"Pilih Model\")\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "outputs = [\n",
        "    gr.Label(num_top_classes=len(le.classes_), label=\"Probabilitas Tiap Kelas\"),\n",
        "    gr.Textbox(label=\"Prediksi Label\")\n",
        "]\n",
        "\n",
        "demo = gr.Interface(fn=predict_bot, inputs=inputs, outputs=outputs, title=\"Deteksi Bot dengan GRU & LSTM\")\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "NX5IQqg7W4qt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "4ce84cae-218a-457e-facc-edbd2d633b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a1b9a499db18575dd5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a1b9a499db18575dd5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}