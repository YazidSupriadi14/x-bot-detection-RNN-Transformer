{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzgKyeqq5Dqg",
        "outputId": "de6285c1-e92f-41d0-feab-540f2e37ce75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bot-detection-mbert'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 47 (delta 13), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (47/47), 1.60 MiB | 4.38 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/yazidsupriadi/bot-detection-mbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "4O9zun_Ulwmz",
        "outputId": "f9e5f50b-c14a-4c5f-fe05-4345d66aea58"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2059551411.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoderConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m from .configuration_auto import (\n\u001b[1;32m     42\u001b[0m     \u001b[0mCONFIG_MAPPING_NAMES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsdp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_fsdp_managed_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasking_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_masks_for_generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misin_mps_friendly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtensionsTrie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/masking_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_torch_greater_or_equal_than_2_6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace_wrapped_higher_order_op\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformGetItemToIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallbackTrigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sympy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmpmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     raise ImportError(\"SymPy now depends on mpmath as an external library. \"\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mpmath/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0musertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mctx_fp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFPContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mctx_mp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mctx_iv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPIntervalContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mpmath/ctx_fp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# ======== Model Definition ========\n",
        "class mBERTBotClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_numerical_features):\n",
        "        super(mBERTBotClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc_text = nn.Linear(self.bert.config.hidden_size, 128)\n",
        "        self.fc_numeric = nn.Linear(num_numerical_features, 32)\n",
        "        self.classifier = nn.Linear(128 + 32, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerical_features):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.last_hidden_state[:, 0, :]  # [CLS]\n",
        "        text_feat = self.dropout(self.fc_text(pooled_output))\n",
        "        numeric_feat = self.fc_numeric(numerical_features)\n",
        "        combined = torch.cat((text_feat, numeric_feat), dim=1)\n",
        "        logits = self.classifier(combined)\n",
        "        return logits.squeeze()\n",
        "\n",
        "# ======== Load Assets ========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "model = mBERTBotClassifier(bert_model=bert_model, num_numerical_features=5)\n",
        "model.load_state_dict(torch.load(\"./bot-detection-mbert/pytorch_model.bin\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "scaler = joblib.load(\"./bot-detection-mbert/scaler.pkl\")\n",
        "\n",
        "# ======== Predict Function ========\n",
        "def predict(username, text, favorite_count, retweet_count, reply_count, quote_count, tweet_per_day, model_choice):\n",
        "    # Tokenize text input\n",
        "    tokens = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "    # Scale numeric features\n",
        "    numeric = np.array([[tweet_per_day, favorite_count, retweet_count, reply_count, quote_count]])\n",
        "    numeric_scaled = scaler.transform(numeric)\n",
        "    numeric_tensor = torch.tensor(numeric_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        raw_logits = model(input_ids, attention_mask, numeric_tensor)\n",
        "\n",
        "        # Ambil satu nilai float logit saja\n",
        "        if isinstance(raw_logits, torch.Tensor):\n",
        "            raw_logits = raw_logits.squeeze().item()\n",
        "\n",
        "        # Hitung probabilitas\n",
        "        prob = torch.sigmoid(torch.tensor(raw_logits)).item()\n",
        "        prob = float(max(1e-6, min(1 - 1e-6, prob)))  # clamp agar stabil\n",
        "\n",
        "    label = \"\ud83e\udd16 Bot\" if prob > 0.5 else \"\ud83d\udc64 Human\"\n",
        "    confidence_pct = prob * 100.0\n",
        "\n",
        "    # format output\n",
        "    return f\"\"\"\n",
        "### \ud83d\udd0d Hasil Prediksi Akun\n",
        "**Username:** @{username}\n",
        "**Model:** {model_choice}\n",
        "\n",
        "**Prediksi:** {label}\n",
        "**Confidence:** {confidence_pct:.2f}%\n",
        "\n",
        "\ud83e\udde0 Berdasarkan analisis teks dan aktivitas akun, sistem memperkirakan akun ini **{'bot otomatis' if prob > 0.5 else 'akun manusia'}** dengan tingkat keyakinan **{confidence_pct:.2f}%**.\n",
        "\"\"\"\n",
        "\n",
        "# ======== Gradio Interface ========\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Username\", placeholder=\"@akun_x\"),\n",
        "        gr.Textbox(label=\"Teks Tweet\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Favorite Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Retweet Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Reply Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Quote Count\"),\n",
        "        gr.Slider(0.0, 100.0, step=0.1, label=\"Tweet Per Day\"),\n",
        "        gr.Radio([\"mBERT\"], label=\"Pilih Model\")\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"Hasil Prediksi\"),\n",
        "    title=\"\ud83e\udd16 Deteksi Akun Bot (mBERT + Fitur Numerik)\",\n",
        "    description=(\n",
        "        \"Prediksi apakah akun termasuk **bot atau manusia** berdasarkan teks tweet dan \"\n",
        "        \"fitur aktivitas seperti retweet, reply, dan quote count.\"\n",
        "    ),\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TckGUdNl-je",
        "outputId": "0bd64946-876c-48b5-ab77-f00d683945d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mbert_gru_bot'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 55 (delta 18), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (55/55), 91.94 KiB | 3.28 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/yazidsupriadi/mbert_gru_bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705,
          "referenced_widgets": [
            "038d7ea0cf364116afd830500785e076",
            "2cd7e2cae8ad4488a81704064b166514",
            "b20b1df7f09142039f4314280dc59308",
            "5e623df99f964e2e80f4480f4594b583",
            "dd54ac942d8844bf93c8fe994aef4e2c",
            "28c1d9e4b6e7476d8e4b01cfc232d026",
            "8e5a98ad4d24481abe517f9e48d80c50",
            "7b07d8cc4b9a4af3b1ead35802666b31",
            "709c2c11f99f4e83a79ce6fc186c1945",
            "66086aac8536434195acfba48923c8eb",
            "f4c785099ae34a48b484c168d6c64a8f"
          ]
        },
        "id": "Txd2prl05P9o",
        "outputId": "24bea90b-081e-4102-f41e-0624e65fbabe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "038d7ea0cf364116afd830500785e076"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://86b627f98eb1f9b3b1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://86b627f98eb1f9b3b1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# ======== Model Definition ========\n",
        "class mBERT_GRU(nn.Module):\n",
        "    def __init__(self, numeric_input_dim):\n",
        "        super(mBERT_GRU, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        self.gru = nn.GRU(self.bert.config.hidden_size, 128, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.numeric_fc = nn.Linear(numeric_input_dim, 32)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 2 + 32, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numeric_feats):\n",
        "        with torch.no_grad():\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "        gru_output, _ = self.gru(bert_output)\n",
        "        gru_output = gru_output[:, -1, :]  # Ambil output terakhir\n",
        "\n",
        "        numeric_out = torch.relu(self.numeric_fc(numeric_feats))\n",
        "\n",
        "        combined = torch.cat((gru_output, numeric_out), dim=1)\n",
        "\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        logits = self.fc2(x)\n",
        "        return self.sigmoid(logits)\n",
        "\n",
        "# ======== Load Assets ========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "model = mBERT_GRU(numeric_input_dim=5)\n",
        "model.load_state_dict(torch.load(\"./mbert_gru_bot/model_gru.pt\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "scaler = joblib.load(\".//mbert_gru_bot/scaler.pkl\")\n",
        "\n",
        "# ======== Predict Function ========\n",
        "def predict(username, text, favorite_count, retweet_count, reply_count, quote_count, tweet_per_day, model_choice):\n",
        "    # Tokenize text input\n",
        "    tokens = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "    # Scale numeric features\n",
        "    numeric = np.array([[tweet_per_day, favorite_count, retweet_count, reply_count, quote_count]])\n",
        "    numeric_scaled = scaler.transform(numeric)\n",
        "    numeric_tensor = torch.tensor(numeric_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        prob = model(input_ids, attention_mask, numeric_tensor).item()\n",
        "        prob = max(1e-6, min(1 - 1e-6, prob))  # avoid 0 or 1\n",
        "        label = \"Bot\" if prob > 0.5 else \"Human\"\n",
        "\n",
        "    return f\"\"\"\ud83d\udc64 Username: @{username}\n",
        "\ud83d\udce6 Model: {model_choice}\n",
        "\ud83e\udde0 Prediction: {label}\n",
        "\ud83d\udd22 Confidence: {prob:.4f}\"\"\"\n",
        "\n",
        "# ======== Gradio Interface ========\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Username\", placeholder=\"@akun_x\"),\n",
        "        gr.Textbox(label=\"Teks Tweet\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Favorite Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Retweet Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Reply Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Quote Count\"),\n",
        "        gr.Slider(0.0, 100.0, step=0.1, label=\"Tweet Per Day\"),\n",
        "        gr.Radio([\"mBERT + GRU\"], label=\"Pilih Model\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Hasil Prediksi\"),\n",
        "    title=\"Deteksi Akun Bot (mBERT + GRU + Fitur Numerik)\",\n",
        "    description=\"Prediksi apakah sebuah akun merupakan bot berdasarkan teks tweet dan fitur aktivitas.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yjORxPkDPtq",
        "outputId": "359fe544-33d6-474e-e158-0ea978ee5c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indo_gru_bot'...\n",
            "remote: Enumerating objects: 277, done.\u001b[K\n",
            "remote: Counting objects: 100% (274/274), done.\u001b[K\n",
            "remote: Compressing objects: 100% (274/274), done.\u001b[K\n",
            "remote: Total 277 (delta 109), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Receiving objects: 100% (277/277), 363.74 KiB | 4.60 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "Filtering content: 100% (6/6), 1.88 GiB | 38.34 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/yazidsupriadi/indo_gru_bot/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872,
          "referenced_widgets": [
            "af88f2a96b4448a9bf64974803f11690",
            "95134d1df6ab46268a7451d4b76ba344",
            "c58d7a28334a4fa8b19ca2dfd01bb35b",
            "f72531f874b2422496372e8f8416ab6b",
            "ea8f966f999d4cc39313c7d20c7b281c",
            "2e7cd7e059bf48f7be08bb4a2a3ea47d",
            "cdf03bdaef454d0182adecd19762d459",
            "757c4fd036cc47b4942cf0cb32ed208a",
            "b917114ecdfd431c88b65c411b0770f0",
            "13ca71be6ba54f668b663b48fa812b26",
            "b75c498bc44a40c4baccb401748166a4",
            "42709f33c535448b840d174fca83a3f3",
            "9bbaa7aabe8f473db225d0192385ac87",
            "6782b012eac24e06bcf23999cdbccbbf",
            "4cd6d398ca0b4071984905f622c0c1af",
            "d94c7f4d92b945529196aa200ce1a4c1",
            "64474a9f96014fbcb768082170560b4b",
            "d45ae037f69744a7b08c7afa8d4ba6c6",
            "be0c65e031c5446eb0579ee474c24f1c",
            "93f76f3fb99c4726a492f2a858e77624",
            "ffcffa06d81d42b9a46d0c601eddc1c1",
            "a6cc5925bda641d1a8d84c6535a2b9f6",
            "cc3b316ba3774c88bddc3567e4d5aaba",
            "8361d47506b74c4db771d43ae7be674e",
            "54e0cd654fc941e1b9cc03a67046ebf8",
            "669d07ec6dbc4a5593c41473e8d76b75",
            "84ffd44249fb43d4a63f55b14422672a",
            "81d800f4f4e74ed5a991ee2eda6299cc",
            "6d1a2ca151c648d3a7579de098ee249a",
            "39bfcfd0ce23460e974bc683fc8f342b",
            "8b828e8ee495469c9a2313aefb096347",
            "21fdbd34fa5e4248b26888bf5c061e4e",
            "2652b0ea06bf4ab08c5dc2d6493d9eda",
            "adf2fa45a5584742be422b3cf35cbbef",
            "2f7ac0e7b7ab40b5b82ef891fdbd5781",
            "ea00b0fb3d9d4a5cb4a51de499dd48bd",
            "b997e1ce128c45cb991063d08ba92bf8",
            "3a229b7a7efa4d7ca5621c0eff12f23e",
            "7d41aea22d434531a97b13ba2e26da80",
            "0d0df3dfe5c14f309bcb0d8d210aab11",
            "e65952c13ab54752989c22455dcbeef7",
            "766edde3418a4ea0bf1f5eb98fa3e8d6",
            "f77eb6dbbee14de1a54acd07466eb3fc",
            "c71e1172b80f49db9ff453a62c4be0a2",
            "6033f297920f4692805eff93722330e7",
            "1dfbf8e4a185413f8c1f434f333c35de",
            "be1803b94d7d470cb266dbfb4bd13dd9",
            "b0d53d571b5f4622bc927dd8264b0732",
            "0e7f749a5b7c4b65968b7e853989aa27",
            "6c457fcd880e424581d11132b962181b",
            "050a9271afdf48c9b7f4fb2f469d8abf",
            "98854071d21a460e8469cb7869a9a545",
            "dfb22517ed19428f995fce812f2c1fbe",
            "fe98146ee2024b2685936bd57025afa0",
            "4c4bb10a23cc49059e4d3298c9fb9b1b",
            "8736ec08fb4b4215941708c360c0dd69",
            "169fe878c32c42bfb08aef4d26b38a63",
            "17529acf41c74d81bac2cab567e787de",
            "54b60958eb1d4a7c87df4143bb606f9a",
            "eed006b804884271aa847788386a8a68",
            "f0d9be5825e541c78347438200bf3744",
            "de81bfd1b1bd45519949e4f7657f152e",
            "27804b9bdeef42a3b2c88c1e47e65325",
            "31a153ebe10b4fc388014d9fbb0babdc",
            "3d76db10c37a4c59927f98b9056576e3",
            "ffc53304e43646979dcef0bb7c454766"
          ]
        },
        "id": "jyqBq3YV6wBH",
        "outputId": "184e69a6-089a-49ec-e13a-425ec0f6f071"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af88f2a96b4448a9bf64974803f11690",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42709f33c535448b840d174fca83a3f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc3b316ba3774c88bddc3567e4d5aaba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adf2fa45a5584742be422b3cf35cbbef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6033f297920f4692805eff93722330e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8736ec08fb4b4215941708c360c0dd69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bfa27b346ca907e71e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://bfa27b346ca907e71e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# ======== Model Definition IndoBERT + GRU ========\n",
        "class IndoBERT_GRU(nn.Module):\n",
        "    def __init__(self, numeric_input_dim):\n",
        "        super(IndoBERT_GRU, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "        self.gru = nn.GRU(self.bert.config.hidden_size, 128, batch_first=True)  # hanya GRU 1 arah\n",
        "\n",
        "        self.numeric_fc = nn.Linear(numeric_input_dim, 32)\n",
        "        self.fc1 = nn.Linear(128 + 32, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numeric_feats):\n",
        "        with torch.no_grad():\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "        gru_output, _ = self.gru(bert_output)\n",
        "        gru_output = gru_output[:, -1, :]  # ambil last hidden state\n",
        "\n",
        "        numeric_out = torch.relu(self.numeric_fc(numeric_feats))\n",
        "\n",
        "        combined = torch.cat((gru_output, numeric_out), dim=1)\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        logits = self.fc2(x)\n",
        "        return self.sigmoid(logits)\n",
        "\n",
        "# ======== Load Assets ========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "model = IndoBERT_GRU(numeric_input_dim=5)\n",
        "model.load_state_dict(torch.load(\"./indo_gru_bot/indobert_gru_bot_detection.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "scaler = joblib.load(\"./indo_gru_bot/scaler.pkl\")\n",
        "\n",
        "# ======== Predict Function ========\n",
        "def predict(username, text, favorite_count, retweet_count, reply_count, quote_count, tweet_per_day, model_choice):\n",
        "    # Tokenize text input\n",
        "    tokens = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "    # Scale numeric features\n",
        "    numeric = np.array([[tweet_per_day, favorite_count, retweet_count, reply_count, quote_count]])\n",
        "    numeric_scaled = scaler.transform(numeric)\n",
        "    numeric_tensor = torch.tensor(numeric_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        prob = model(input_ids, attention_mask, numeric_tensor).item()\n",
        "        prob = max(1e-6, min(1 - 1e-6, prob))  # avoid 0 or 1\n",
        "        label = \"Bot\" if prob > 0.5 else \"Human\"\n",
        "\n",
        "    return f\"\"\"\ud83d\udc64 Username: @{username}\n",
        "\ud83d\udce6 Model: {model_choice}\n",
        "\ud83e\udde0 Prediction: {label}\n",
        "\ud83d\udd22 Confidence: {prob:.4f}\"\"\"\n",
        "\n",
        "# ======== Gradio Interface ========\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Username\", placeholder=\"@akun_x\"),\n",
        "        gr.Textbox(label=\"Teks Tweet\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Favorite Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Retweet Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Reply Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Quote Count\"),\n",
        "        gr.Slider(0.0, 100.0, step=0.1, label=\"Tweet Per Day\"),\n",
        "        gr.Radio([\"IndoBERT + GRU\"], label=\"Pilih Model\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Hasil Prediksi\"),\n",
        "    title=\"Deteksi Akun Bot (IndoBERT + GRU + Fitur Numerik)\",\n",
        "    description=\"Prediksi apakah sebuah akun merupakan bot berdasarkan teks tweet dan fitur aktivitas.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}