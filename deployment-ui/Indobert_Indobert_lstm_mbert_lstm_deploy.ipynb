{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK66E3Ow5cX4",
        "outputId": "c4b202a7-2714-4bf3-dd74-3844d11f7107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'bot-detection-indobert'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 72 (delta 0), reused 0 (delta 0), pack-reused 69 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (72/72), 1.91 MiB | 9.14 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/yazidsupriadi/bot-detection-indobert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 187,
          "referenced_widgets": [
            "1ecb9d9578df483493080017633a54bd",
            "24ffb81048f8412ea3f5ecc88197a9e9",
            "158c8f219bb7470ea101b9b0dbb77078",
            "80fe66d670f2401d88a65debf7eb3a58",
            "d8ca82aabffc47c1a802609c84047a57",
            "1ea1e56b32654de0a25038a5f8189f99"
          ]
        },
        "id": "4O9zun_Ulwmz",
        "outputId": "b07108ba-5d0e-4571-8c0e-d06c30edf6ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ecb9d9578df483493080017633a54bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24ffb81048f8412ea3f5ecc88197a9e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "158c8f219bb7470ea101b9b0dbb77078",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80fe66d670f2401d88a65debf7eb3a58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8ca82aabffc47c1a802609c84047a57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ea1e56b32654de0a25038a5f8189f99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model, tokenizer, scaler loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://99637b3f76fb065e47.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://99637b3f76fb065e47.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import joblib\n",
        "import gradio as gr\n",
        "\n",
        "# ================================\n",
        "# Define model\n",
        "# ================================\n",
        "class IndoBERTBotClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IndoBERTBotClassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768 + 5, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerical_features):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.last_hidden_state[:, 0, :]\n",
        "        combined = torch.cat((pooled_output, numerical_features), dim=1)\n",
        "        logits = self.classifier(combined)\n",
        "        return logits.squeeze()\n",
        "\n",
        "# ================================\n",
        "# Load tokenizer, model, scaler\n",
        "# ================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "model = IndoBERTBotClassifier()\n",
        "state_dict = torch.load(\"./bot-detection-indobert/pytorch_model.bin\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "scaler = joblib.load(\"./bot-detection-indobert/scaler.pkl\")\n",
        "\n",
        "print(\"‚úÖ Model, tokenizer, scaler loaded successfully.\")\n",
        "\n",
        "# ================================\n",
        "# Prediction function\n",
        "# ================================\n",
        "def predict_gradio(text, favorite_count, retweet_count, reply_count, quote_count, tweet_per_day, model_choice):\n",
        "    numeric_features = [tweet_per_day, favorite_count, retweet_count, reply_count, quote_count]\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "    # Scale numeric\n",
        "    numeric_scaled = scaler.transform(np.array([numeric_features]))\n",
        "    numeric_tensor = torch.tensor(numeric_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask, numeric_tensor)\n",
        "        prob = torch.sigmoid(logits).item()\n",
        "        label = \"Bot\" if prob > 0.5 else \"Human\"\n",
        "\n",
        "    return f\"\"\"üìù Model: {model_choice}\n",
        "üìå Prediction: {label}\n",
        "üî¢ Confidence: {prob:.4f}\"\"\"\n",
        "\n",
        "# ================================\n",
        "# Gradio interface\n",
        "# ================================\n",
        "demo = gr.Interface(\n",
        "    fn=predict_gradio,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Teks Tweet\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Favorite Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Retweet Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Reply Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Quote Count\"),\n",
        "        gr.Slider(0.0, 100.0, step=0.1, label=\"Tweet Per Day\"),\n",
        "        gr.Radio([\"IndoBERT\"], label=\"Pilih Model\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Hasil Prediksi\"),\n",
        "    title=\"Deteksi Akun Bot (IndoBERT + Fitur Numerik)\",\n",
        "    description=\"Prediksi apakah sebuah akun merupakan bot berdasarkan teks tweet dan fitur aktivitas.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TckGUdNl-je",
        "outputId": "ffdb0c34-077f-4341-ea16-e7d63fcb3f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'indo_lstm_bot'...\n",
            "remote: Enumerating objects: 581, done.\u001b[K\n",
            "remote: Counting objects: 100% (578/578), done.\u001b[K\n",
            "remote: Compressing objects: 100% (578/578), done.\u001b[K\n",
            "remote: Total 581 (delta 269), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Receiving objects: 100% (581/581), 542.62 KiB | 8.48 MiB/s, done.\n",
            "Resolving deltas: 100% (269/269), done.\n",
            "Filtering content: 100% (3/3), 953.09 MiB | 35.78 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/yazidsupriadi/indo_lstm_bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "hXDQOHEDx4Uk",
        "outputId": "ebcc7385-d033-4fe4-9293-a02a485d6597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ IndoBERT + LSTM model loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a4a5a46a7023a98047.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a4a5a46a7023a98047.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "class IndoBERT_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IndoBERT_LSTM, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "        self.lstm = nn.LSTM(self.bert.config.hidden_size, 128, batch_first=True)\n",
        "        self.classifier = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerical_features=None):\n",
        "        with torch.no_grad():\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        _, (hidden, _) = self.lstm(bert_output)\n",
        "        lstm_out = hidden[-1]\n",
        "        logits = self.classifier(lstm_out)\n",
        "        return self.sigmoid(logits).squeeze()\n",
        "\n",
        "# ================================\n",
        "# Load assets\n",
        "# ================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "model = IndoBERT_LSTM()\n",
        "model.load_state_dict(torch.load(\"./indo_lstm_bot/model_epoch_10.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ IndoBERT + LSTM model loaded successfully.\")\n",
        "\n",
        "# ================================\n",
        "# Prediction function\n",
        "# ================================\n",
        "def predict_gradio(text, favorite_count, retweet_count, reply_count, quote_count, tweet_per_day, model_choice):\n",
        "    # Input numerik tetap diterima untuk konsistensi UI tapi diabaikan\n",
        "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prob = model(input_ids, attention_mask).item()\n",
        "    label = \"Bot\" if prob > 0.5 else \"Human\"\n",
        "\n",
        "    return f\"\"\"üìù Model: {model_choice}\n",
        "üìå Prediction: {label}\n",
        "üî¢ Confidence: {prob:.4f}\"\"\"\n",
        "\n",
        "# ================================\n",
        "# Gradio Interface\n",
        "# ================================\n",
        "demo = gr.Interface(\n",
        "    fn=predict_gradio,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Teks Tweet\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Favorite Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Retweet Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Reply Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Quote Count\"),\n",
        "        gr.Slider(0.0, 100.0, step=0.1, label=\"Tweet Per Day\"),\n",
        "        gr.Radio([\"IndoBERT +LSTM\"], label=\"Pilih Model\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Hasil Prediksi\"),\n",
        "    title=\"Deteksi Akun Bot (IndoBERT + LSTM + Fitur Numerik)\",\n",
        "    description=\"Prediksi apakah sebuah akun merupakan bot berdasarkan teks tweet dan fitur aktivitas.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoD82vLUEnIs",
        "outputId": "5eba3c00-d340-4a0c-c324-5d8ba8dcedbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mbert_lstm_bot'...\n",
            "remote: Enumerating objects: 322, done.\u001b[K\n",
            "remote: Counting objects: 100% (319/319), done.\u001b[K\n",
            "remote: Compressing objects: 100% (319/319), done.\u001b[K\n",
            "remote: Total 322 (delta 141), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Receiving objects: 100% (322/322), 514.48 KiB | 5.65 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "Filtering content: 100% (3/3), 1.32 GiB | 21.98 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/yazidsupriadi/mbert_lstm_bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "FH-x8NRoEb9k",
        "outputId": "cdb7067a-a449-4ceb-81ea-f7170c3db1d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ mBERT + LSTM model loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://38c9112bc2c9e52815.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://38c9112bc2c9e52815.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import joblib\n",
        "import gradio as gr\n",
        "\n",
        "# =========================================\n",
        "# MODEL DEFINITION: mBERT + LSTM + NUMERIC\n",
        "# =========================================\n",
        "class mBERT_LSTM(nn.Module):\n",
        "    def __init__(self, num_numerical_features):\n",
        "        super(mBERT_LSTM, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        self.lstm = nn.LSTM(self.bert.config.hidden_size, 128, batch_first=True)\n",
        "\n",
        "        self.num_fc = nn.Sequential(\n",
        "            nn.Linear(num_numerical_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(128 + 32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, numerical_features):\n",
        "        with torch.no_grad():\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        _, (hidden, _) = self.lstm(bert_output)\n",
        "        lstm_out = hidden[-1]\n",
        "\n",
        "        num_out = self.num_fc(numerical_features)\n",
        "        combined = torch.cat((lstm_out, num_out), dim=1)\n",
        "        logits = self.classifier(combined)\n",
        "        return self.sigmoid(logits).squeeze()\n",
        "\n",
        "# ================================\n",
        "# LOAD MODEL, TOKENIZER, SCALER\n",
        "# ================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "model = mBERT_LSTM(num_numerical_features=5)\n",
        "state_dict = torch.load(\"./mbert_lstm_bot/model_epoch_10.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "scaler = joblib.load(\"./mbert_lstm_bot/scaler.pkl\")\n",
        "print(\"‚úÖ mBERT + LSTM model loaded successfully.\")\n",
        "\n",
        "# ================================\n",
        "# INFERENCE FUNCTION\n",
        "# ================================\n",
        "def predict_gradio(text, favorite_count, retweet_count, reply_count, quote_count, tweet_per_day, model_choice):\n",
        "    numeric_features = [tweet_per_day, favorite_count, retweet_count, reply_count, quote_count]\n",
        "    numeric_scaled = scaler.transform(np.array([numeric_features]))\n",
        "    numeric_tensor = torch.tensor(numeric_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prob = model(input_ids, attention_mask, numeric_tensor).item()\n",
        "    label = \"Bot\" if prob > 0.5 else \"Human\"\n",
        "\n",
        "    return f\"\"\"üìù Model: {model_choice}\n",
        "üìå Prediction: {label}\n",
        "üî¢ Confidence: {prob:.4f}\"\"\"\n",
        "\n",
        "# ================================\n",
        "# GRADIO INTERFACE\n",
        "# ================================\n",
        "demo = gr.Interface(\n",
        "    fn=predict_gradio,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Teks Tweet\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Favorite Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Retweet Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Reply Count\"),\n",
        "        gr.Slider(0, 10000, step=1, label=\"Quote Count\"),\n",
        "        gr.Slider(0.0, 100.0, step=0.1, label=\"Tweet Per Day\"),\n",
        "        gr.Radio([\"mBERT + LSTM\"], label=\"Pilih Model\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Hasil Prediksi\"),\n",
        "    title=\"Deteksi Bot (mBERT + LSTM + Fitur Numerik)\",\n",
        "    description=\"Prediksi akun bot di platform X menggunakan model mBERT + LSTM dengan fitur aktivitas akun.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}